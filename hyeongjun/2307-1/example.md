김형준 
## 자연어 처리 주요 논문 공유
|논문 명|설명|설명|
|------|---|---|
|GloVe: Global vectors for word representation|EMNLP|Word2Vec의 단점을 보완하여 global co-occurence statics를 기반으로 단어의 representation을 학습하는 모델 제안|
|Neural Machine Translation by Jointly Learning to Align and Translate|arXiv|Attention mechanism을 사용하여 단순 RNN/LSTM을 이용한 번역 모델보다 뛰어난 성능을 내보냄|
|Attention Is All You Need|Neurips|추후에 나올 GPT와 BERT의 근간이 되는 논문으로 Attention 기법을 이용하여 Translation Task의 성능을 비약적으로 끌어올림|
|Improving Language Understanding by Generative Pre-Training|arXiv|OpenAI에서 개발한 GPT 모델 transformer를 이용해서 SOTA를 달성|
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understatnding|arXiv|트랜스포머의 인코더만을 이용하여 문장의 단어의 포지션과 관계를 학습하여 SOTA를 달성  |
|Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks|arXiv|sentence transformers|
Learning Transferable Visual Models From Natural Language Supervision|arXiv|CLIP|
|Language Models are Few-Shot Learners|arXiv|GPT3|
|LLaMA:Open and Efficient Foundation Language Models|arXiv|llama|
|Proximal Policy Optimization Algorithms|arXiv|PPO|
|Training language models to follow instructions with human feedback|arXiv|설명|
|GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers|arXiv|설명|
